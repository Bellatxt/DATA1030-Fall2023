{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>DATA1030: Hands-on Data Science</center>\n",
    "## <center>Intro to ML</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"figures/1030.png\" width=\"600\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mud card\n",
    "\n",
    "### Admin\n",
    "- **I was wondering how broad our options are for the data we choose for our final project, and what our final goal can be.**\n",
    "    - any topic is fine\n",
    "    - the dataset should one of these three properties:\n",
    "        - contains missing values\n",
    "        - not i.i.d. (it is time series or one object is described by multiple datapoints)\n",
    "        - the dataset is large\n",
    "    - use your dataset to answer a regression or a classification question\n",
    "- **I also want to know the grade criterion of our individual final project. What kinds of aspect decide the grade? For example, the fluency, completeness and logicality of our essay, or the data we are going to use, etc.**\n",
    "    - it's a mix of all of those aspects\n",
    "    - we will grade based on the technical correctness of your work but we will also take into account the quality of the reports and the presentations\n",
    "    - rubrics will be sent out ahead of time\n",
    "- **I struggle with how to open the GitHub documents with Jupyter Notebook.**\n",
    "    - you will have problems by the time you submit the first problem set :)\n",
    "- **What is the difference between this class and other CSCI classes offered like Machine Learning?**\n",
    "    - CSCI1420 and DATA2060 are equivalent classes\n",
    "    - both are much more theoretical than DATA1030\n",
    "    - we learn about the math and numerics behind ML algorithms and the students implement it using python and numpy using OOP\n",
    "- **How much background coding experience is needed in Python for this class?**\n",
    "    - if you know basic container types, control flow, and know how to write functions, you will be fine \n",
    "- **For the individual semester project, is the project individual only for the writing of the code? Or everything needs to be completely individual?**\n",
    "    - completely individual\n",
    "    - not sure how a group of people would give one joint presentation if everyone works on a different dataset :)\n",
    "- **Why we're not doing deep learning.**\n",
    "    - this is a mandatory course for the DSI master's students and they are required to take the deep learning course CSCI2470\n",
    "- **Will there be examples of projects from previous semesters that we will be able to look at before getting started on our own?**\n",
    "    - yes, they are already in the course's github repo\n",
    "\n",
    "### Supervised ML\n",
    "- **Is y a true label for one feature or a group if features. E.g if you have a structure dataset with house #bed, #bath, price, and sq. Ft, what feature would Y be the true label for?**\n",
    "    - y would be the sale price of the house\n",
    "    - one price per house and each house is described by a number of features\n",
    "- **Difference between classification and regression problems.**\n",
    "    - we will cover this much more\n",
    "    - classification: your target variable is categorical (e.g., yes-no, grades like A, B, C, NC)\n",
    "    - regression: the target variable is continuous (e.g., sale price, people's age)\n",
    "- **The different types of variables and are they all considered the same weight.**\n",
    "    - initially yes because you might not know or you might be wrong about the best way to weight them\n",
    "    - some ML algorithms will assign weights\n",
    "- **I think you mentioned hyper-parameters? Not really sure what that is**\n",
    "    - that's OK, give it 4-5 weeks :)\n",
    "- **The amount of data needed to achieve high performance (before there is a plateau)? It seemed that there was some way of finding this out and I would be interested to know how to test this in practice**\n",
    "    - yep, it's called the learning curve and we will cover it during the second half of the term\n",
    " \n",
    "### Other ML areas \n",
    "- **I just got a little bit confused about the first sentence \"only the feature matrix X is available, there is no target variable\" in the part of Other Areas of ML. What does it mean?**\n",
    "    - there is no target variable to predict in unsupervised ML\n",
    "- **The muddiest part of the lecture was the mechanisms of unsupervised learning and the instances of when it would be helpful. I understand in theory when and why one would want to find groups and cluster data, but I would love to see an example of how those groups may be formed and its uses.**\n",
    "    - we might cover some of this towards the end but unsupervised ML is not the main topic of this lecture\n",
    "    - do some online reading if you'd like to learn more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "By the end of the lecture, you will be able to\n",
    "- describe the main goals of the ML pipeline \n",
    "- list the main steps of the ML pipeline\n",
    "- explain the bias-variance trade off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='LIGHTGRAY'> Learning objectives </font>\n",
    "<font color='LIGHTGRAY'>By the end of the lecture, you will be able to </font>\n",
    "- **describe the main goals of the ML pipeline**\n",
    "- <font color='LIGHTGRAY'>list the main steps of the ML pipeline </font>\n",
    "- <font color='LIGHTGRAY'>explain the bias-variance trade off </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An ML example\n",
    "- let's assume you just moved to an island and you never had papayas before but it is common on the island\n",
    "- what do you do? \n",
    "- sample some papayas and collect some info\n",
    "    - for each papaya you try, you collect color, firmness, and whether it tasted good or not\n",
    "    - classification problem with two features\n",
    "- once you have enough data, you can train a machine learning model to predict if a new previously unseen papaya is tasty or not based on its color and firmness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's define this problem!\n",
    "- **the learner's input**\n",
    "    - Domain set $\\mathcal{X}$ - a set of objects we wish to label. In the papaya example: the set of all papayas. $\\mathcal{X}$ can be in infinite set or a set that's too large to handle on any computer (e.g., all possible 640x480 images with 3 color channels and 256 possible pixel values)\n",
    "        - domain points are represented by a vector of features e.g., (color, firmness)\n",
    "        - domain points are also called instances, and $\\mathcal{X}$ is also called the instance space\n",
    "    - Label set $\\mathcal{Y}$ - a set of possible labels. In the papaya example: we restrict our label set to {0,1}, 0 meaning the papaya tastes bad, 1 meaning the papaya tastes good. \n",
    "        - such a label set is categorical, i.e., we have a classification problem at hand\n",
    "        - the label set can be continuous too, e.g., the real number between 0 and 1, meaning that 0.5 is an OK tasting papaya.\n",
    "        - the label set can also be probabilistic\n",
    "            - i.e., two papayas with the same color and firmness can sometimes be tasty and sometimes bad\n",
    "            - this is quite normal, the features you collect usually do not uniquely determine the label\n",
    "    - Training data $S = ((x_1, y_1),...,(x_m,y_m))$ - a finite sequence of pairs from $\\mathcal{X}$, $\\mathcal{Y}$. This is what the learner has access to.\n",
    "        - $S$ is also called the training set, and examples in $S$ are also called training examples\n",
    "        - $X = (x_1,...,x_m)$ is the feature matrix which is usually a 2D matrix, and $Y = (y_1,...,y_m)$ is the target variable which is a vector.\n",
    "        \n",
    "        \n",
    "        \n",
    "- **the learner's output**\n",
    "    - a prediction rule $h: \\mathcal{X} \\rightarrow \\mathcal{Y}$ - this is also called the predictor, a hypothesis, or in the papaya example a classifier. It would be a regressor if $\\mathcal{Y}$ was continuous. In the papaya example, the predictor is the rule that our learner will employ to predict if a papaya will be tasty based on color and firmness as they examine it e.g., in the farmer's market or before picking the fruit from the tree. \n",
    "    - this prediction rule is generated based on $S$ so $h: X \\rightarrow Y$ is more appropriate\n",
    "    - once the prediction rule is determined, we can use it to predict the label to previously unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is $S$ used?\n",
    "- in ML, you only use part of $S$ to train the model\n",
    "- you hold out some fraction of $S$ to calculate what's called the **generalization error**\n",
    "- it measures how well the model is expected to perform on previously unseen data\n",
    "- it helps to avoid models that overfit or underfit\n",
    "    - overfit: model is too complex, it performs very well on the training set but it doesn't generalize to previously unseen data\n",
    "    - underfit: the model is too simple, it performs poorly on te training set and on previously unseen data as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap the goals:\n",
    "- use the training data (X and y) to develop a <font color='red'>model</font> which can <font color='red'>accurately</font> predict the target variable (y_new') for previously unseen data (X_new)\n",
    "    - model performance or 'accuracy' is a metric you need to choose to measure model performance and objectively compare various models\n",
    "- measure the generalization error: measure how well the model is expected to perform on previously unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='LIGHTGRAY'>Learning objectives</font>\n",
    "<font color='LIGHTGRAY'>By the end of the lecture, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>describe the main goals of the ML pipeline </font>\n",
    "- **list the main steps of the ML pipeline**\n",
    "- <font color='LIGHTGRAY'>explain the bias-variance trade off</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The steps\n",
    "\n",
    "**1. Exploratory Data Analysis (EDA)**: you need to understand your data and verify that it doesn't contain errors\n",
    "   - do as much EDA as you can!\n",
    "    \n",
    "**2. Split the data into different sets**: most often the sets are train, validation, and test (or holdout)\n",
    "   - practitioners often make errors in this step!\n",
    "   - you can split the data randomly, based on groups, based on time, or any other non-standard way if necessary to answer your ML question\n",
    "\n",
    "**3. Preprocess the data**: ML models only work if X and Y are numbers! Some ML models additionally require each feature to have 0 mean and 1 standard deviation (standardized features)\n",
    "   - often the original features you get contain strings (for example a gender feature would contain 'male', 'female', 'non-binary', 'unknown') which needs to be transformed into numbers\n",
    "   - often the features are not standardized (e.g., age is between 0 and 100) but it needs to be standardized\n",
    "    \n",
    "**4. Choose an evaluation metric**: depends on the priorities of the stakeholders\n",
    "   - often requires quite a bit of thinking and ethical considerations\n",
    "     \n",
    "**5. Choose one or more ML techniques**: it is highly recommended that you try multiple models\n",
    "   - start with simple models like linear or logistic regression\n",
    "   - try also more complex models like nearest neighbors, support vector machines, random forest, etc.\n",
    "    \n",
    "**6. Tune the hyperparameters of your ML models (aka cross-validation)**\n",
    "   - ML techniques have hyperparameters that you need to optimize to achieve best performance\n",
    "   - for each ML model, decide which parameters to tune and what values to try\n",
    "   - loop through each parameter combination\n",
    "       - train one model for each parameter combination\n",
    "       - evaluate how well the model performs on the validation set\n",
    "   - take the parameter combo that gives the best validation score\n",
    "   - evaluate that model on the test set to report how well the model is expected to perform on previously unseen data\n",
    "    \n",
    "**7. Interpret your model**: black boxes are often not useful\n",
    "   - check if your model uses features that make sense (excellent tool for debugging)\n",
    "   - often model predictions are not enough, you need to be able to explain how the model arrived to a particular prediction (e.g., in health care)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='LIGHTGRAY'>Learning objectives</font>\n",
    "<font color='LIGHTGRAY'>By the end of the lecture, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>describe the main goals of the ML pipeline </font>\n",
    "- <font color='LIGHTGRAY'>list the main steps of the ML pipeline</font>\n",
    "- **explain the bias-variance trade off**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-variance tradeoff illustrated through a simple ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from matplotlib import pylab as plt\n",
    "import matplotlib\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "\n",
    "# scikit-learn code is reproducable is the random seed is fixed.\n",
    "np.random.seed(2)\n",
    "\n",
    "# read in the data\n",
    "# our toy dataset, we don't know how it was generated.\n",
    "df = pd.read_csv('data/toy_data.csv')\n",
    "\n",
    "X = df[['x1','x2']].values\n",
    "y = df['y'].values\n",
    "\n",
    "print(np.shape(X))\n",
    "print(np.shape(y))\n",
    "print(np.unique(y,return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Exploratory Data Analysis (EDA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X[y==0,0],alpha=0.4,color='r',label='x1 for class 0')\n",
    "plt.hist(X[y==1,0],alpha=0.4,color='b',label='x1 for class 1')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('y')\n",
    "plt.title('toy dataset')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X[y==0,1],alpha=0.4,color='r',label='x2 for class 0')\n",
    "plt.hist(X[y==1,1],alpha=0.4,color='b',label='x2 for class 1')\n",
    "plt.xlabel('x2')\n",
    "plt.ylabel('y')\n",
    "plt.title('toy dataset')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[y==0,0],X[y==0,1],color='r',label='class 0',alpha=0.4)\n",
    "plt.scatter(X[y==1,0],X[y==1,1],color='b',label='class 1',alpha=0.4)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('toy dataset')\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Split the data into different sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_other, y_train, y_other = train_test_split(X,y,test_size=0.4)\n",
    "print(np.shape(X_other),np.shape(y_other))\n",
    "print('train:',np.shape(X_train),np.shape(y_train))\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,test_size=0.5)\n",
    "print('val:',np.shape(X_val),np.shape(y_val))\n",
    "print('test:',np.shape(X_test),np.shape(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Preprocess the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "# the scaler object contains the feature means and variations in the training set\n",
    "print(scaler.mean_)\n",
    "print(scaler.var_)\n",
    "\n",
    "# the scaler is used to transform the sets\n",
    "X_train_prep = scaler.transform(X_train)\n",
    "X_val_prep = scaler.transform(X_val)\n",
    "X_test_prep = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Choose an evaluation metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Choose one or more ML techniques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(SVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Tune the hyperparameters of your ML models (aka cross-validation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.logspace(-1,3,13)\n",
    "print(Cs)\n",
    "train_scores = []\n",
    "validation_scores = []\n",
    "models = []\n",
    "for C in Cs:\n",
    "    classifier = SVC(kernel='rbf',C = C, probability=True) # this is our classifier\n",
    "    classifier.fit(X_train_prep,y_train) # the model is fitted to the training data\n",
    "    \n",
    "    y_train_pred = classifier.predict(X_train_prep)\n",
    "    train_accuracy = accuracy_score(y_train,y_train_pred) # calculate the validation accuracy\n",
    "    train_scores.append(train_accuracy)\n",
    "    \n",
    "    y_val_pred = classifier.predict(X_val_prep) # predict the validation set\n",
    "    validation_accuracy = accuracy_score(y_val,y_val_pred) # calculate the validation accuracy\n",
    "    validation_scores.append(validation_accuracy)\n",
    "    \n",
    "    models.append(classifier)\n",
    "    print(C, train_accuracy, validation_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The bias - variance tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Cs,train_scores,label='train score')\n",
    "plt.plot(Cs,validation_scores,label='validation score')\n",
    "plt.semilogx()\n",
    "plt.legend()\n",
    "plt.xlabel('C parameter')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **high bias model** (aka underfitting)\n",
    "    - it performs poorly on the train and validation sets\n",
    "    - small C values in the example above\n",
    "- **high variance model** (aka overfitting)\n",
    "    - it performs very well on the training set but it performs poorly on the validation set\n",
    "    - high C\n",
    "- the goal of the parameter tuning is to find the balance between bias and variance\n",
    "    - usually the best model is the one with the best validation score\n",
    "    - C = 46 in our case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the best model perform on the test set?\n",
    "- this score tells us how well the model generalizes to previously unseen data because the test set was not touched before\n",
    "- usually it is close to the best validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = models[-5].predict(X_test_prep)\n",
    "print(accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Interpret your model**\n",
    "- with two features, this is easy\n",
    "- plot the decision boundary and probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "h = .02  # step size in the mesh\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# use the best model with C = 46\n",
    "classifier = models[-5] \n",
    "# scale the data before predicting! this is very important!\n",
    "Z = classifier.predict_proba(scaler.transform(np.c_[xx.ravel(), yy.ravel()]))[:, 1] \n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contour(xx, yy, Z,vmin=0,vmax=1,levels=[0.5],colors=['k'])\n",
    "plt.contourf(xx, yy, Z, cmap=cm, alpha=.8,vmin=0,vmax=1,levels=np.arange(0,1.02,0.02))\n",
    "plt.colorbar(ticks=[0,0.2,0.4,0.6,0.8,1],label='predicted probability')\n",
    "plt.scatter(X_train[y_train==0,0],X_train[y_train==0,1],color='r',label='class 0')\n",
    "plt.scatter(X_train[y_train==1,0],X_train[y_train==1,1],color='b',label='class 1')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('the decision boundary')\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.savefig('figures/decision_boundary.jpg',dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "Hopefully you can now\n",
    "- describe the main goals of the ML pipeline \n",
    "- list the main steps of the ML pipeline\n",
    "- explain the bias-variance trade off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mud card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
